\section{Introduction}

\noindent As moving to a new age, people tend to need more communication. Filling the gap between different cultures requires translating countless books, articles and words. Human resource is always a limitation. Thousands of 
pioneers have developed a serious of Mathematical model for doing translation mechanically. 

Much of the previous model employ too much mathematics,
	focus not enough to the true meaning of the words.
Translate is not a simple mapping from source language to target language.
In order to do perfect translation,the true meaning of the words must be understood. 

Making a computer program understand a string is the so called
	 \hbox{\emph{Lexical}} 	\hbox{\emph{Analyse}}. 
When Lexical Analyse finished, the given string will be converted into a abstract syntax tree. 
This tree represents the internal meaning and structure of the analysed string.
Since it's a data struct, programs will find it more easier to do further analyse.
\nocite{Compilers_Principles_Techniques_and_Tools}


Before doing the Lexical Analyse, the string should be tokenized and the every token should be marked by it's part of speech.
This process is called, syntax analyse.

\section{Body}

\subsection{Tokenize}
Before doing any other thing, a subsequence of text need to be divided into tokens\footnote{You can consider tokens as words, but tokens is far more than just words.\stepcounter{footnote}\footnotemark}.
\footnotetext{Punctuations are also tokens.}
A token is an unbreakable group of symbols.

Tokenize can be performed by regular expression. In English, tokens are separated by spaces.
Lex\cite{Lex} helps write programs whose control flow is directed by instances of regular expressions in the input stream. Lex source is a table of regular expressions and corresponding program fragments. The table is translated to a program which reads an input stream, copying it to an output stream and partitioning the input into strings which match the given expressions. As each such string is recognized the corresponding program fragment is executed. The recognition of the expressions is performed by a deterministic finite automaton generated by Lex. The program fragments written by the user are executed in the order in which the corresponding regular expressions occur in the input stream. 

\subsection{Part-of-speech Tagging}
 mostly part-of-speech can be done by a simple lookup in the dictionary.
However, a lot of words have several part of speech.
So, the first difficult thing is to mark the part-of-speech exactly.
Fortunately, we are standing on the shoulder of the giant. 
HMM\footnote{Hidden Markov model}{\citep{HMM_Based_Part_Of_Speech_Tagging}} shows a way.
HMM is first introduced by \emph{Leonard E. Baum} in a series of statistical papers in the second half of the 1960s.




\section{Conclusion}

So, it is highly possible  to write a program that understand the real meanings of the English text.
